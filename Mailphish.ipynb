{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76d8895-024f-4c97-ae49-a4e07eb6de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb8e6873-ef95-452c-96bf-365ac35c5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d48f90c1-84ce-43ca-88ee-579be3a25c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords for text processing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a8164a-75a3-4675-8c26-ac7885909a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam directory: C:\\Users\\USER\\Desktop\\sample_project\\spam\n",
      "Ham directory: C:\\Users\\USER\\Desktop\\sample_project\\easy_ham\n",
      "Spam files found: 501\n",
      "Ham files found: 2501\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "spam_dir = 'spam/'  # Update with actual path\n",
    "ham_dir = 'easy_ham/'  # Update with actual path\n",
    "\n",
    "print(\"Spam directory:\", os.path.abspath(spam_dir))\n",
    "print(\"Ham directory:\", os.path.abspath(ham_dir))\n",
    "\n",
    "print(\"Spam files found:\", len(os.listdir(spam_dir)))\n",
    "print(\"Ham files found:\", len(os.listdir(ham_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a465612d-c710-4696-bc15-76c124f92a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails loaded: 3002\n"
     ]
    }
   ],
   "source": [
    "def load_emails_from_directory(directory, label):\n",
    "    \"\"\"Loads all email files from a given directory and assigns a label.\"\"\"\n",
    "    emails = []\n",
    "    for file_name in os.listdir(directory):  # Load all files, regardless of extension\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            with open(file_path, 'r', encoding='latin-1', errors='ignore') as file:\n",
    "                emails.append(file.read())\n",
    "    return pd.DataFrame({'text': emails, 'label': label})\n",
    "\n",
    "spam_emails = load_emails_from_directory(spam_dir, label=1)\n",
    "ham_emails = load_emails_from_directory(ham_dir, label=0)\n",
    "\n",
    "df = pd.concat([spam_emails, ham_emails], ignore_index=True)\n",
    "print(f\"Total emails loaded: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3de4e8-562f-497a-bf9b-4aefc8a1dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to clean and preprocess email text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5e103e-2726-46fd-9a57-8e76592da21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    2501\n",
      "1    1002\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count phishing (1) and non-phishing (0) emails\n",
    "spam_df = df[df['label'] == 1]  # Get phishing emails\n",
    "df_balanced = pd.concat([df, spam_df], ignore_index=True)  # Duplicate phishing emails\n",
    "\n",
    "# Shuffle dataset to mix original and duplicated emails\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the new distribution\n",
    "print(df_balanced['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb4bc961-dd82-47c1-9c86-0327dd91bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature transformation complete! Training samples: 2802\n"
     ]
    }
   ],
   "source": [
    "# Convert text into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(df_balanced['text'])\n",
    "y = df_balanced['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X = vectorizer.fit_transform(df['text'])\n",
    "#y = df['label']  # Labels (1 = spam, 0 = non-spam)\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Feature transformation complete! Training samples:\", X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68449483-f7e7-46ed-a12f-2d1a432b3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9872\n"
     ]
    }
   ],
   "source": [
    "# Train the NaÃ¯ve Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train Logistic Regression model\n",
    "#model = LogisticRegression(max_iter=500, class_weight='balanced')  # Auto-adjusts weights\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "#model = LogisticRegression(max_iter=500)\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c498f11-a86f-448d-9557-6f468be7da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate âœ… (Confidence: 0.58)\n"
     ]
    }
   ],
   "source": [
    "def classify_email(email_text):\n",
    "    email_text = preprocess_text(email_text)\n",
    "    email_vector = vectorizer.transform([email_text])\n",
    "    prediction = model.predict(email_vector)[0]\n",
    "    confidence = model.predict_proba(email_vector)[0][prediction]  # Get confidence score\n",
    "    return f\"{'Phishing/Spam ðŸš¨' if prediction == 1 else 'Legitimate âœ…'} (Confidence: {confidence:.2f})\"\n",
    "\n",
    "new_email = \"\"\"Congratulations! You've won a $1000 gift card. Click here to claim now.\"\"\"\n",
    "print(classify_email(new_email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39760377-776c-4774-917d-b5cbdb0edc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing ðŸš¨ (Confidence: 0.42)\n"
     ]
    }
   ],
   "source": [
    "def classify_email(email_text, threshold=0.4):  # Lower threshold to 40%\n",
    "    email_text = preprocess_text(email_text)\n",
    "    email_vector = vectorizer.transform([email_text])\n",
    "    proba = model.predict_proba(email_vector)[0][1]  # Probability of phishing\n",
    "    prediction = 1 if proba >= threshold else 0\n",
    "    return f\"{'Phishing ðŸš¨' if prediction == 1 else 'Legitimate âœ…'} (Confidence: {proba:.2f})\"\n",
    "\n",
    "print(classify_email(new_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a8f8a87-d49b-44aa-8a17-41ac95d013ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model and vectorizer\n",
    "joblib.dump(model, \"email_classifier.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96817880-c576-4d56-917b-ecd96d4047cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: flask-cors in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from flask) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\desktop\\sample_project\\env\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06d1fa-0fb1-4294-9962-a72c199d1f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.100.59:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [19/Mar/2025 13:50:21] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:20] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:20] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:25] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:25] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:26] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:26] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:27] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:27] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:28] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:29] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:29] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:30] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:30] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:31] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:31] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:32] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:33] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:34] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:34] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:35] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:36] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:37] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:37] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:38] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:38] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:39] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:40] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:40] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:41] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:41] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:42] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:42] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:43] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:43] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:44] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:44] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:45] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:45] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:46] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:01:46] \"POST /classify HTTP/1.1\" 400 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:18:37] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:18:38] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:18:38] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:18:39] \"POST /classify HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2025 14:18:39] \"POST /classify HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# âœ… Load the trained model and vectorizer\n",
    "model = joblib.load(\"email_classifier.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "@app.route(\"/classify\", methods=[\"POST\"])\n",
    "def classify_email():\n",
    "    try:\n",
    "        data = request.json  # Get JSON data\n",
    "        email_text = data.get(\"text\", \"\")\n",
    "\n",
    "        if not email_text:\n",
    "            return jsonify({\"error\": \"No email text provided\"}), 400\n",
    "\n",
    "        # Transform the email text using the vectorizer\n",
    "        email_features = vectorizer.transform([email_text])\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = model.predict(email_features)[0]\n",
    "        confidence = model.predict_proba(email_features).max()\n",
    "\n",
    "        # Convert prediction to label\n",
    "        label = \"Phishing ðŸš¨\" if prediction == 1 else \"Legitimate âœ…\"\n",
    "\n",
    "        return jsonify({\"label\": label, \"confidence\": round(float(confidence), 2)})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Disable the auto-restart issue in Jupyter\n",
    "    import os\n",
    "    os.environ[\"FLASK_RUN_FROM_CLI\"] = \"false\"\n",
    "\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=False)  # Turn off debug mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c03db-6af5-4fc8-85dd-23ddc587f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4db6e-89f0-4439-8fb7-3ee2bb0a4af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
